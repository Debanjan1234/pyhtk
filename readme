-----------------------------------------------------------------------------------
-- PYHTK                                                                         --
-- A Python package for building GMM-HMM models for speech recognition using HTK --
--                                                                               --
-- Initial code written by: Daniel Gillick (dgillick@gmail.com)                  --
--                                                                               --
-- Code (.py files) licensed under the New BSD License                           --
--    (http://www.opensource.org/licenses/bsd-license.php)                       --
-----------------------------------------------------------------------------------

To create a model:

1. Use make_setup.py or your own script to create a setup file. Look at the
examples in the Setups directory. Each line consists of an audio file, its
transcription, and a config file used to process the audio.

2. Create a config file, using the examples in the Configs directory as
templates. Configs/si84.config is a training config, while Configs/nov92.config
is a testing config. See model.py to understand in more detail what each
variable in the config file means.

3. Put a pronunciation dictionary in the Common directory. The CMU dictionary is
available here:
https://cmusphinx.svn.sourceforge.net/svnroot/cmusphinx/trunk/cmudict/cmudict.0.7a

4. Make sure the project dependencies are setup properly.
  - Python 2.5+
  - HTK 3.4
  - SRILM
  - sph2pipe

You should be able to run these tools from the command line, so make sure they're
in your path.

5. Run model.py to build a model. For example:

python model.py Configs/si84.config

6. Test your model. For example:

python test.py -g 8 -i 6 Configs/si84.config Configs/nov92.config
> gives WER: 13.55

python test.py -g 8 -i 6 -m Configs/si84.config Configs/nov92.config
> gives WER: 12.65

Note that by default, the testing code is ignoring over 100 of the test utterances
(330 -> 216) because they contain at least one word that wasn't in the training
data.
The WSJ corpus ships with a standard 5k dictionary and LM. Using these, the
WER is 8.59 with 8 MLE-trained Gaussians, and 7.81 using MPE.
